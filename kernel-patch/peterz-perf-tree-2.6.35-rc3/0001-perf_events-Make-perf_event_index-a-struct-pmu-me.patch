From 088032004c4b93cfd2b9c07653ccf44d800faa61 Mon Sep 17 00:00:00 2001
From: Jakub Ruzicka <ruzicka.jakub@gmail.com>
Date: Wed, 11 Aug 2010 11:00:10 +0200
Subject: [PATCH] perf_events: Make perf_event_index() a struct pmu method

---
 arch/powerpc/kernel/perf_event.c |   14 ++++++++++++++
 arch/x86/kernel/cpu/perf_event.c |   14 ++++++++++++++
 include/linux/perf_event.h       |    1 +
 kernel/perf_event.c              |   15 ++-------------
 4 files changed, 31 insertions(+), 13 deletions(-)

diff --git a/arch/powerpc/kernel/perf_event.c b/arch/powerpc/kernel/perf_event.c
index af1d9a7..40203c3 100644
--- a/arch/powerpc/kernel/perf_event.c
+++ b/arch/powerpc/kernel/perf_event.c
@@ -850,6 +850,19 @@ static void power_pmu_unthrottle(struct perf_event *event)
 }
 
 /*
+ * Return the event index that can be read from userspace
+ * through mmap and used with RDPMC.
+ */
+static int power_pmu_index(struct perf_event *event)
+{
+   if (event->state != PERF_EVENT_STATE_ACTIVE)
+       return 0;
+
+   return event->hw.idx; /* +1 (0 means N/A) -1 (power specific offset) */
+}
+
+
+/*
  * Start group events scheduling transaction
  * Set the flag to make pmu::enable() not perform the
  * schedulability test, it will be performed at commit time
@@ -906,6 +919,7 @@ struct pmu power_pmu = {
 	.disable	= power_pmu_disable,
 	.read		= power_pmu_read,
 	.unthrottle	= power_pmu_unthrottle,
+	.index		= power_pmu_index,
 	.start_txn	= power_pmu_start_txn,
 	.cancel_txn	= power_pmu_cancel_txn,
 	.commit_txn	= power_pmu_commit_txn,
diff --git a/arch/x86/kernel/cpu/perf_event.c b/arch/x86/kernel/cpu/perf_event.c
index f2da20f..f7ae15c 100644
--- a/arch/x86/kernel/cpu/perf_event.c
+++ b/arch/x86/kernel/cpu/perf_event.c
@@ -1022,6 +1022,19 @@ static void x86_pmu_unthrottle(struct perf_event *event)
 	WARN_ON_ONCE(ret);
 }
 
+static int x86_pmu_index(struct perf_event *event)
+{
+	int index = event->hw.idx;
+
+	if (event->state != PERF_EVENT_STATE_ACTIVE)
+		return 0;
+
+	if (index >= 32)
+		index = (index - 32) | (1 << 30);
+
+	return index + 1;
+}
+
 void perf_event_print_debug(void)
 {
 	u64 ctrl, status, overflow, pmc_ctrl, pmc_count, prev_left, fixed;
@@ -1457,6 +1470,7 @@ static const struct pmu pmu = {
 	.stop		= x86_pmu_stop,
 	.read		= x86_pmu_read,
 	.unthrottle	= x86_pmu_unthrottle,
+	.index		= x86_pmu_index,
 	.start_txn	= x86_pmu_start_txn,
 	.cancel_txn	= x86_pmu_cancel_txn,
 	.commit_txn	= x86_pmu_commit_txn,
diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 9073bde..aeda54c 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -565,6 +565,7 @@ struct pmu {
 	void (*stop)			(struct perf_event *event);
 	void (*read)			(struct perf_event *event);
 	void (*unthrottle)		(struct perf_event *event);
+	int (*index)			(struct perf_event *event);
 
 	/*
 	 * Group events scheduling is treated as a transaction, add group
diff --git a/kernel/perf_event.c b/kernel/perf_event.c
index 403d180..f42b721 100644
--- a/kernel/perf_event.c
+++ b/kernel/perf_event.c
@@ -2320,18 +2320,6 @@ int perf_event_task_disable(void)
 	return 0;
 }
 
-#ifndef PERF_EVENT_INDEX_OFFSET
-# define PERF_EVENT_INDEX_OFFSET 0
-#endif
-
-static int perf_event_index(struct perf_event *event)
-{
-	if (event->state != PERF_EVENT_STATE_ACTIVE)
-		return 0;
-
-	return event->hw.idx + 1 - PERF_EVENT_INDEX_OFFSET;
-}
-
 /*
  * Callers need to ensure there can be no nesting of this function, otherwise
  * the seqlock logic goes bad. We can not serialize this because the arch
@@ -2356,7 +2344,7 @@ void perf_event_update_userpage(struct perf_event *event)
 	preempt_disable();
 	++userpg->lock;
 	barrier();
-	userpg->index = perf_event_index(event);
+	userpg->index = event->pmu->index(event);
 	userpg->offset = perf_event_count(event);
 	if (event->state == PERF_EVENT_STATE_ACTIVE)
 		userpg->offset -= local64_read(&event->hw.prev_count);
@@ -4297,6 +4285,7 @@ static const struct pmu perf_ops_generic = {
 	.start		= perf_swevent_int,
 	.stop		= perf_swevent_void,
 	.read		= perf_swevent_read,
+	.index		= perf_swevent_int,
 	.unthrottle	= perf_swevent_void, /* hwc->interrupts already reset */
 };
 
-- 
1.5.5.6

